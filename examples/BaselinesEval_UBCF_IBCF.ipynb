{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UBCF & IBCF - MultiEval Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the example from the LKPY package as a template. Modifications are made as necessary, but this is largely a copy of the example given with the LKPY package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We first need to import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'C:\\\\Users\\\\Jacob\\\\Documents\\\\GitHub\\\\lenskit_confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenskit.batch import MultiEval\n",
    "from lenskit.crossfold import partition_users, SampleN, partition_netflix\n",
    "from lenskit.algorithms import basic, als, item_knn, user_knn\n",
    "from lenskit.datasets import MovieLens, Netflix\n",
    "from lenskit import topn, util #, metrics\n",
    "from lenskit.metrics import predict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress bars are useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a little while to run things, and can get kinda quiet in here. Let's set up logging so we can see the logging output in the notebook's message stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.log_to_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set up the data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlsmall = MovieLens('../data/ml-latest-small')\n",
    "#mlsmall = MovieLens('../data/ml-1m')\n",
    "#mlsmall = MovieLens('../data/ml-10m')\n",
    "#mlsmall = MovieLens('../data/ml-20m')\n",
    "data = Netflix('../data/netflix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to run our evaluation and store its output in the `my-eval` directory, generating 20-item recommendation lists::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = MultiEval('my-eval', predict = True, recommend = 100, eval_n_jobs = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a 5-fold cross-validation setup.  We save the data into a list in memory so we have access to the test data later.  In a larger experiment, you might write the partitions to disk and pass the file names to `add_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(partition_netflix(data))\n",
    "#pairs = list(partition_users(mlsmall.ratings, 5, SampleN(5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.add_datasets(pairs, name = 'Netflix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhbr_range = [10, 25, 50, 75] #, 50] #, 75] #, 200] #, 50, 75, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.add_algorithms([user_knn.UserUser(nnbrs = f, aggregate = 'weighted-average') for f in nhbr_range], \n",
    "                    attrs = ['nnbrs'], name = 'UserKNN-Weighted')\n",
    "\n",
    "eval.add_algorithms([user_knn.UserUser(nnbrs = f, aggregate = 'average') for f in nhbr_range], \n",
    "                    attrs = ['nnbrs'], name = 'UserKNN-Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.add_algorithms([item_knn.ItemItem(nnbrs = f, aggregate = 'weighted-average') for f in nhbr_range], \n",
    "                    attrs = ['nnbrs'], name = 'ItemKNN-Weighted')\n",
    "\n",
    "eval.add_algorithms([item_knn.ItemItem(nnbrs = f, aggregate = 'average') for f in nhbr_range], \n",
    "                    attrs = ['nnbrs'], name = 'ItemKNN-Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we will run the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval.run(progress = tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now that the experiment is run, we can read its outputs.\n",
    "\n",
    "First the run metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = pd.read_csv('my-eval/runs.csv')\n",
    "runs.set_index('RunId', inplace = True)\n",
    "runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This describes each run - a data set, partition, and algorithm combination.  To evaluate, we need to get the actual recommendations, and combine them with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.read_parquet('my-eval/recommendations.parquet')\n",
    "#del recs['RunId']\n",
    "recs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.read_parquet('my-eval/predictions.parquet')\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to compute per-(run,user) evaluations of the recommendations *before* combining with metadata. \n",
    "\n",
    "In order to evaluate the recommendation list, we need to build a combined set of truth data. Since this is a disjoint partition of users over a single data set, we can just concatenate the individual test frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = pd.concat((p.test for p in pairs), ignore_index = True)\n",
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up an analysis and compute the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rla = topn.RecListAnalysis()\n",
    "rla.add_metric(topn.ndcg) # precision, recall, recip_rank, dcg, ndcg\n",
    "rla.add_metric(topn.precision)\n",
    "#rla.add_metric(predict.rmse)\n",
    "raw_ndcg = rla.compute(recs, truth)\n",
    "raw_ndcg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to combine this with our run data, so that we know what algorithms and configurations we are evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR NEIGHBORHOOD-BASED METHODS ONLY ###\n",
    "ndcg = raw_ndcg.join(runs[['name', 'nnbrs']], on = 'RunId')\n",
    "ndcg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the overall average performance for each algorithm configuration - fillna makes the group-by happy with Popular's lack of a feature count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### FOR NEIGHBORHOOD-BASED METHODS ONLY ###\n",
    "ndcg.fillna(0).groupby(['name', 'nnbrs'])['ndcg','precision'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR NEIGHBORHOOD-BASED METHODS ONLY ###\n",
    "scores = ndcg.groupby(['name', 'nnbrs'])['ndcg'].mean().reset_index()\n",
    "#pop_score = ndcg[ndcg['AlgoClass'] == 'Popular']['ndcg'].mean()\n",
    "#plt.axhline(pop_score, color='grey', linestyle='--', label='Popular')\n",
    "for algo, data in scores.groupby('name'):\n",
    "    plt.plot(data['nnbrs'], data['ndcg'], label=algo)\n",
    "    \n",
    "#plt.yticks(np.arange(0.002, 0.011, 0.001))\n",
    "plt.legend()\n",
    "plt.xlabel('nnbrs')\n",
    "plt.ylabel('nDCG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR NEIGHBORHOOD-BASED METHODS ONLY ###\n",
    "scores = ndcg.groupby(['name', 'nnbrs'])['precision'].mean().reset_index()\n",
    "#pop_score = ndcg[ndcg['AlgoClass'] == 'Popular']['ndcg'].mean()\n",
    "#plt.axhline(pop_score, color='grey', linestyle='--', label='Popular')\n",
    "for algo, data in scores.groupby('name'):\n",
    "    plt.plot(data['nnbrs'], data['precision'], label=algo)\n",
    "    \n",
    "#plt.yticks(np.arange(0.0015, 0.006, 0.0005))\n",
    "plt.legend()\n",
    "plt.xlabel('nnbrs')\n",
    "plt.ylabel('Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truth # user, item, rating, timestamp - 3355\n",
    "#preds # RunId, user, item rating, prediction\n",
    "\n",
    "### FOR NEIGHBORHOOD-BASED METHODS ONLY ###\n",
    "pred_acc = preds.join(runs[['name', 'nnbrs']], on = 'RunId')\n",
    "pred_acc.head()\n",
    "\n",
    "\n",
    "#from lenskit.metrics.predict import rmse\n",
    "#rmse(preds['prediction'], preds['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_acc.loc[pred_acc['prediction'] > 5,'prediction'] = 5\n",
    "#pred_acc.loc[pred_acc['prediction'] < 1,'prediction'] = 1\n",
    "\n",
    "pred_acc['se'] = (pred_acc['rating'] - pred_acc['prediction'])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(pred_acc.groupby(['name', 'nnbrs'])['se'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR NEIGHBORHOOD-BASED METHODS ONLY ###\n",
    "knn_pred_scores = np.sqrt(pred_acc.groupby(['name', 'nnbrs'])['se'].mean()).reset_index()\n",
    "knn_pred_scores.head()\n",
    "#pop_score = ndcg[ndcg['AlgoClass'] == 'Popular']['ndcg'].mean()\n",
    "#plt.axhline(pop_score, color='grey', linestyle='--', label='Popular')\n",
    "for algo, data in knn_pred_scores.groupby('name'):\n",
    "    plt.plot(data['nnbrs'], data['se'], label=algo)\n",
    "plt.legend()\n",
    "plt.xlabel('nnbrs')\n",
    "plt.ylabel('RMSE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
