{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for processing TopN results for Baseline Models UBCF, IBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # set path of locally install lenskit_confidence module\n",
    "sys.path.insert(0,'C:\\\\Users\\\\Name\\\\Documents\\\\GitHub\\\\lenskit_confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lenskit import topn, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "util.log_to_notebook() # let notebook print updates from functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'ml1m' # Specify dataset to process - 'ml1m', 'ml10m', 'ml20m', 'jester', etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:\\\\results_conf_aware_nbhd_KDD2021\\\\' + data + '\\\\baselines\\\\' # where are results stored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lengths = list((1,2,3,4,5,6,7,8,9,10)) # what list size are we looking at? \n",
    "# For example, if you produce 10 recommendations per user, we want to look at metrics@k for k in {1,...,10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output information from the models runs\n",
    "# Should be in the \"path\" directory specified above\n",
    "runs = pd.read_csv(path + 'runs.csv') \n",
    "runs.set_index('RunId', inplace = True)\n",
    "recs = pd.read_parquet(path + 'recommendations.parquet')\n",
    "recs = recs[['item', 'score', 'user','rank','RunId']]\n",
    "recs = recs[recs['rank'] <= list_length]\n",
    "truth = pd.read_csv(path + 'truth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for list_length in list_lengths: # a simple loop to compute TopN results for each list length\n",
    "\n",
    "    print(list_length) # just to keep track, some of the \n",
    "    rla = topn.RecListAnalysis()\n",
    "    rla.add_metric(topn.ndcg) \n",
    "    rla.add_metric(topn.precision)\n",
    "    topn_compute = rla.compute(recs, truth)\n",
    "    topn_results = topn_compute.join(runs[['name', 'nnbrs']], on = 'RunId')\n",
    "    topn_results.fillna(0).groupby(['name', 'nnbrs'])['ndcg','precision'].mean().to_csv(path + 'results' + str(eval('list_length')) + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
